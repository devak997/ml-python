{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Sai Teja Gudapati\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.linalg import pinv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are predicting profits for a foodchain based on a city's population\n",
    "# Data is read from the file into a pandas DataFrame\n",
    "data = pd.read_csv('./foodchain.txt')\n",
    "\n",
    "#Read the feature matrix into X\n",
    "X = data.population #or data.iloc[:,0]\n",
    "\n",
    "#Read the output vector into the vector y\n",
    "y = data.profit #or data.iloc[:,1]\n",
    "\n",
    "#Read number of training examples into m\n",
    "m = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Since our problem has only one feature, we can visualize the data using a simple 2D scatter plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X,y,ls='', marker = 'o', label='population vs profit')\n",
    "ax.set(xlabel='population in 10000s', ylabel='profits in 10000$', title='population vs profit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we have only one feature population, we are required to fit two parameters theta0 and theta1\n",
    "### the hypothesis function would be h(x) = theat0*x0 + theta1*x1\n",
    "\n",
    "### theta is the vector that contains theta0 and theta1\n",
    "### here x1 is the population feature. x0 feature for every training example contains 1 recall y=mx+c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((2,1)) #initialize theta0 and theta1 to zeros. We will initialize theta values randomly in practice\n",
    "n = len(theta) # no of parameters\n",
    "\n",
    "#Add the x0 feature column that contains ones\n",
    "X = np.array(X).reshape(m,n-1) #convert X to np array\n",
    "y = np.array(y).reshape(m,1)\n",
    "X = np.hstack((np.ones((m,1)), X)) # add a column of ones before population column in X\n",
    "\n",
    "iterations = 1500 #no of iterations for training\n",
    "alpha = 0.01 #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cost Function\n",
    "def computeCost(X, y, theta, m):\n",
    "    sum=0\n",
    "    for item in range(m):\n",
    "        sum += (np.dot(X[item],theta) - y[item])**2\n",
    "    return sum/(2*m)\n",
    "#print(computeCost(X,y,theta)[0]) #This value should be approximately 32.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Gradient Descent\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    cost_history = np.zeros((num_iters,1))\n",
    "    X_trans = np.transpose(X)\n",
    "    m = len(y)\n",
    "    for item in range(num_iters):\n",
    "        theta -= ((alpha/m) * np.dot(X_trans,(np.dot(X,theta) - y)))\n",
    "        cost_history[item] = computeCost(X,y,theta,m)[0]\n",
    "    return theta,cost_history\n",
    "theta,cost_his = gradientDescent(X,y,theta,alpha,iterations) #theta will contain final values of parameters\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the linear fit after obtaining final values of theta\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(data.population, data.profit,ls='', marker = 'o', label='population vs profit')\n",
    "ax.plot(data.population, np.dot(X,theta))\n",
    "ax.set(xlabel='population in 10000s', ylabel='profits in 10000$', title='population vs profit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the cost function is varying\n",
    "theta0_vals = np.arange(-10,10,0.2)\n",
    "theta1_vals = np.arange(-1,4, 0.05)\n",
    "\n",
    "cost_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "        t = np.array([theta0_vals[i], theta1_vals[j]]).reshape(2,1)\n",
    "        cost_vals[i][j] = computeCost(X,y,t,m)\n",
    "theta0_vals,theta1_vals = np.meshgrid(theta0_vals, theta1_vals)\n",
    "\n",
    "ax.plot_surface(X = theta0_vals,Y= theta1_vals, Z =cost_vals)\n",
    "ax.set(xlabel='theta0', ylabel='theta1', zlabel='cost', title='cost vs theta')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict profits for given population using our model\n",
    "def predict(population):\n",
    "    pop = population/10000;\n",
    "    feature_vector = np.array([1, pop]).reshape(1,2)\n",
    "    profit = np.dot(feature_vector, theta)[0][0]\n",
    "    return profit*10000\n",
    "print(predict(35000)) #prints profit when population of a city is 35000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with multiple variables (Multivariate Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are predicting prices of houses\n",
    "# Load the data\n",
    "data1 = pd.read_csv('./house_prices.txt')\n",
    "\n",
    "#input matrix\n",
    "X1 = np.array(data1.iloc[:,:2])\n",
    "\n",
    "#no of training examples\n",
    "m1 = np.shape(X1)[0]\n",
    "\n",
    "#prices vector\n",
    "y1 = np.array(data1.iloc[:,2]).reshape(m1,1)\n",
    "\n",
    "#no of features + 1 i.e., theta0,......thetan\n",
    "n1 = np.shape(X1)[1] + 1\n",
    "\n",
    "#Theta vector\n",
    "theta1 = np.zeros((n1,1))\n",
    "\n",
    "#Display data\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We notice 2 features from above data size and no of bedrooms. The features have a difference in scale by about 1000. To make Gradient Descent Converge faster, we need to scale the features so that their values are comparable. For this purpose, we perform feature scaling. Feature Scaling has been implemented in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Normalization\n",
    "def featureNormalize(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - mu)/sigma\n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "X1,mu,sigma = featureNormalize(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have add a vector of ones to our input matrix to represent the feature x0\n",
    "X1 = np.hstack((np.ones((m1,1)), X1))\n",
    "\n",
    "#Choose learning rate\n",
    "alpha1=0.01\n",
    "\n",
    "# No of iterations before gradient descent stops\n",
    "num_iters = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we perform gradient descent\n",
    "theta1,cost_his1 = gradientDescent(X1,y1,theta1,alpha1,num_iters)\n",
    "print(theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To ensure that gradient descent is converging, we plot the cost after each iteration vs number of iterations. We should see that the curve is decreasing. if you find that the curve is increasing, it means that gradient descent is not converging and that you have chosen a very high learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cost vs number of iterations to know whether gradient descent worked correctly or not\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(list(range(1,401)), cost_his1)\n",
    "ax.set(xlabel='no of iterations', ylabel = 'cost', title='cost vs iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the prices of houses\n",
    "def predict_price(size, bedrooms,theta1):\n",
    "    size = (size - mu[0])/sigma[0] #because we have normalized the features,\n",
    "    bedrooms = (bedrooms - mu[1])/sigma[1]#we have to narmalize size and bedrooms for prediction also\n",
    "    feature_vector = np.array([1, size, bedrooms]).reshape(1,3)\n",
    "    price = np.dot(feature_vector, theta1)[0][0]\n",
    "    return price\n",
    "\n",
    "#For a 1650 sqft 3-bedroom house, the price would be:\n",
    "print(predict_price(1650,3,theta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have used gradient descent. Now we will use another method which basically solves for theta a system of linear equations\n",
    "# The method is called the normal equation method. It is useful only when we have not more than 10 features or so.\n",
    "def normal_equation(X,y):\n",
    "    X_trans = np.transpose(X)\n",
    "    theta1_normal = np.dot(pinv(np.dot(X_trans,X)), np.dot(X_trans,y))\n",
    "    return theta1_normal\n",
    "theta1_normal = normal_equation(X1,y1)\n",
    "\n",
    "#prediction of price using normal equation method\n",
    "print(predict_price(1650,3,theta1_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
